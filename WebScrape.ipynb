{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f0fda5-4a6a-4379-a082-0756b1f5c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = 'https://www.cnn.com'\n",
    "\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "containers = soup.find_all('div', class_=[\n",
    "    'container_lead-package__cards-wrapper', \n",
    "    'container_lead-plus-headlines-with-images__cards-wrapper', \n",
    "    'container_grid-3__cards-wrapper', \n",
    "    'container_lead-plus-headlines__cards-wrapper', \n",
    "    'container_list-headlines-with-images__cards-wrapper', \n",
    "    'container_grid-2__cards-wrapper', \n",
    "    'container_grid-4__cards-wrapper', \n",
    "    'container_list-headlines__cards-wrapper'\n",
    "])\n",
    "\n",
    "\n",
    "articles = []\n",
    "for container in containers:\n",
    "    links = container.find_all('a')\n",
    "    for link in links:\n",
    "        span = link.find('span', class_='container__headline-text')\n",
    "        if span:\n",
    "            headline_text = span.get_text(strip=True)\n",
    "            href = link.get('href')\n",
    "            if href and not href.startswith('http'):\n",
    "                href = f\"https://www.cnn.com{href}\"\n",
    "            if href and href.endswith('index.html'):\n",
    "                href = href[:-10]\n",
    "            articles.append({'headline': headline_text, 'link': href})\n",
    "\n",
    "\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        content_div = article_soup.find('div', class_='article__content-container')\n",
    "        \n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            return content\n",
    "        else:\n",
    "            return None\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching article: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "950848cd-9930-4021-a2f7-139784f83981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/health-fitness/best-meal-prep-containers\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/health-fitness/slip-on-shoes-for-exercise\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/home/essential-cleaning-tools\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/deals/best-after-christmas-sales-2024-12-26\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/reviews/best-tv\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/health-fitness/best-non-alcoholic-beers\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/reviews/best-electric-blankets\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/home/best-amazon-home-decor\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/health-fitness/how-to-whiten-teeth-at-home\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/reviews/best-shoes-for-standing-all-day\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/beauty/best-body-lotions\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/fashion/best-fleece-jackets\n",
      "Error fetching article: 403 Client Error: Forbidden for url: https://www.cnn.com/cnn-underscored/reviews/best-water-fountain-cats-dogs\n",
      "Added 0 new articles to 'cnn_articles.json'.\n",
      "There are 155 total articles present in 'cnn_articles.json'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_existing_articles(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as json_file:\n",
    "            try:\n",
    "                return json.load(json_file)\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Error decoding JSON file.\")\n",
    "                return []\n",
    "    return []\n",
    "\n",
    "\n",
    "file_path = 'cnn_articles.json'\n",
    "\n",
    "\n",
    "existing_articles = load_existing_articles(file_path)\n",
    "existing_links = {article['link'] for article in existing_articles}\n",
    "\n",
    "new_articles_with_content = []\n",
    "for article in articles:\n",
    "    if article['link'] not in existing_links:\n",
    "        content = scrape_article_content(article['link'])\n",
    "        if content:\n",
    "            new_article = {\n",
    "                'headline': article['headline'],\n",
    "                'link': article['link'],\n",
    "                'content': content\n",
    "            }\n",
    "            new_articles_with_content.append(new_article)\n",
    "\n",
    "\n",
    "updated_articles = existing_articles + new_articles_with_content\n",
    "#with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "#    json.dump(updated_articles, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Added {len(new_articles_with_content)} new articles to '{file_path}'.\")\n",
    "print(f\"There are {len(updated_articles)} total articles present in '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "863747bb-f07c-4a73-9fd1-e37f6e6adcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 159 articles to 'fox_articles.json'\n"
     ]
    }
   ],
   "source": [
    "url2 = 'https://www.foxnews.com/'\n",
    "\n",
    "page2 = requests.get(url2)\n",
    "\n",
    "soup2 = BeautifulSoup(page2.text, 'html.parser')\n",
    "\n",
    "containers2 = soup2.find_all('h3', class_='title')\n",
    "\n",
    "articles2 = []\n",
    "for container in containers2:\n",
    "    links = container.find_all('a') \n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href and not href.startswith('http'):\n",
    "            href = f\"https:{href}\"\n",
    "        headline_text = link.get_text(strip=True)\n",
    "        articles2.append({'headline': headline_text, 'link': href})\n",
    "\n",
    "\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "            \n",
    "        article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "        content_div = article_soup.find('div', class_='article-content')\n",
    "            \n",
    "        if content_div:\n",
    "            paragraphs = content_div.find_all('p')\n",
    "            content = ' '.join([p.get_text(strip=True) for p in paragraphs])\n",
    "            return content\n",
    "        else:\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching article: {e}\")\n",
    "        return None\n",
    "\n",
    "articles_with_content2 = []\n",
    "for article in articles2:\n",
    "    content = scrape_article_content(article['link'])\n",
    "    if content:  # Only include articles with valid content\n",
    "        articles_with_content2.append({\n",
    "            'headline': article['headline'],\n",
    "            'link': article['link'],\n",
    "            'content': content\n",
    "        })\n",
    "\n",
    "# Save to a JSON file\n",
    "with open('fox_articles.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(articles_with_content2, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Saved {len(articles_with_content2)} articles to 'fox_articles.json'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6490d81f-c9e7-44b5-aec3-a33c54c6149a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 159 new articles to 'fox_articles.json'.\n",
      "There are 236 total articles present in 'fox_articles.json'.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'fox_articles.json'\n",
    "\n",
    "existing_articles = load_existing_articles(file_path)\n",
    "existing_links = {article['link'] for article in existing_articles}\n",
    "\n",
    "new_articles_with_content2 = []\n",
    "for article in articles2:\n",
    "    if article['link'] not in existing_links:\n",
    "        content = scrape_article_content(article['link'])\n",
    "        if content:\n",
    "            new_article = {\n",
    "                'headline': article['headline'],\n",
    "                'link': article['link'],\n",
    "                'content': content\n",
    "            }\n",
    "            new_articles_with_content2.append(new_article)\n",
    "\n",
    "\n",
    "updated_articles2 = existing_articles + new_articles_with_content2\n",
    "with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(updated_articles2, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Added {len(new_articles_with_content2)} new articles to '{file_path}'.\")\n",
    "print(f\"There are {len(updated_articles2)} total articles present in '{file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1361dd-278e-4d92-bdcf-016bebe8762e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe3ae1b-1cea-41e3-a424-40b15f6b460a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba6b23-a037-49b5-9db9-c9507e5d3fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
