{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de0635e-959b-4ddc-ae88-f8edbc46821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saee/miniforge3/envs/tf_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "import transformers\n",
    "transformers.is_tf_available = lambda: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf4013bb-2227-4f34-880c-1a2eca56a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# json to csv cell \n",
    "import json\n",
    "import csv\n",
    " \n",
    "with open('cnn_articles.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    " \n",
    "articles = data['articles']\n",
    " \n",
    "data_file = open('articles.csv', 'w')\n",
    "\n",
    "csv_writer = csv.writer(data_file)\n",
    "\n",
    "count = 0\n",
    " \n",
    "for article in articles:\n",
    "    if count == 0:\n",
    " \n",
    "        header = article.keys()\n",
    "        csv_writer.writerow(header)\n",
    "        count += 1\n",
    " \n",
    "    csv_writer.writerow(article.values())\n",
    " \n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563326d9-9b23-44a0-ae5a-f0137d7c3acd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              headline  \\\n",
      "0    The president-elect has railed against the pol...   \n",
      "1    6 key lines from Trump’s Sunday speech to cons...   \n",
      "2    Why Ivanka Trump left politics and isn’t comin...   \n",
      "3    Manchin torches Democrats on the way out the door   \n",
      "4    A picture is emerging of Resistance 2.0 as lib...   \n",
      "..                                                 ...   \n",
      "150  Meet the husband-wife legal team representing ...   \n",
      "151  Some inmates seeking education behind bars fac...   \n",
      "152  Costco is pushing back — hard — against the an...   \n",
      "153  Christmas dinner injury forces golf’s World No...   \n",
      "154  Why Nefertiti still inspires, 3,300 years afte...   \n",
      "\n",
      "                                                  link  \\\n",
      "0    https://www.cnn.com/2024/12/22/politics/birthr...   \n",
      "1    https://www.cnn.com/2024/12/22/politics/trump-...   \n",
      "2    https://www.cnn.com/2024/12/22/politics/ivanka...   \n",
      "3    https://www.cnn.com/2024/12/22/politics/joe-ma...   \n",
      "4    https://www.cnn.com/2024/12/22/politics/resist...   \n",
      "..                                                 ...   \n",
      "150  https://www.cnn.com/2024/12/28/us/karen-friedm...   \n",
      "151  https://www.cnn.com/2024/12/28/business/inmate...   \n",
      "152  https://www.cnn.com/2024/12/27/business/costco...   \n",
      "153  https://www.cnn.com/2024/12/27/sport/scottie-s...   \n",
      "154  https://www.cnn.com/2024/12/26/style/nefertiti...   \n",
      "\n",
      "                                               content  \\\n",
      "0    President-elect Donald Trump’s team is assessi...   \n",
      "1    President-elect Donald Trump on Sunday, in his...   \n",
      "2    For Ivanka Trump’s entire adult life, she work...   \n",
      "3    For more on Sen. Joe Manchin of West Virginia,...   \n",
      "4    The question has loomed over Democrats and the...   \n",
      "..                                                 ...   \n",
      "150  The rapper Sean “Diddy” Combs and the suspecte...   \n",
      "151  Graduation day dawns sunny and warm for the fi...   \n",
      "152  Costco is battling an anti-DEI wave with a ste...   \n",
      "153  WhileChristmasDay is the most wonderful time o...   \n",
      "154  One hundred years ago, in a courtyard at the N...   \n",
      "\n",
      "                   category                multi_hot_labels  \n",
      "0                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "1                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "2                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "3                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "4                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "..                      ...                             ...  \n",
      "150  Society, Entertainment  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0]  \n",
      "151       Business, Society  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
      "152                Business  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
      "153                  Sports  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
      "154    Entertainment, World  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0]  \n",
      "\n",
      "[155 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# preprocess data for modle \n",
    "file_path = \"articles.csv\"\n",
    "articles_df = pd.read_csv(file_path)\n",
    "\n",
    "categories = ['Politics', 'Business', 'Health', 'Science', 'Technology', \n",
    "              'Sports', 'Entertainment', 'World', 'Society', 'Everyday']\n",
    "category_to_index = {category: idx for idx, category in enumerate(categories)}\n",
    "\n",
    "def multi_hot_encode(categories_str):\n",
    "    label_vector = np.zeros(len(categories), dtype=int)\n",
    "    if isinstance(categories_str, str): \n",
    "        for cat in categories_str.split(','):\n",
    "            cat = cat.strip() \n",
    "            if cat in category_to_index:  \n",
    "                label_vector[category_to_index[cat]] = 1\n",
    "    return label_vector\n",
    "\n",
    "# multi hot encoder for current labels \n",
    "articles_df['multi_hot_labels'] = articles_df['category'].apply(multi_hot_encode)\n",
    "\n",
    "print(articles_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bcbab60-c3dd-48c3-a71d-e6830abb52ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              headline  \\\n",
      "0    The president-elect has railed against the pol...   \n",
      "1    6 key lines from Trump’s Sunday speech to cons...   \n",
      "2    Why Ivanka Trump left politics and isn’t comin...   \n",
      "3    Manchin torches Democrats on the way out the door   \n",
      "4    A picture is emerging of Resistance 2.0 as lib...   \n",
      "..                                                 ...   \n",
      "150  Meet the husband-wife legal team representing ...   \n",
      "151  Some inmates seeking education behind bars fac...   \n",
      "152  Costco is pushing back — hard — against the an...   \n",
      "153  Christmas dinner injury forces golf’s World No...   \n",
      "154  Why Nefertiti still inspires, 3,300 years afte...   \n",
      "\n",
      "                                                  link  \\\n",
      "0    https://www.cnn.com/2024/12/22/politics/birthr...   \n",
      "1    https://www.cnn.com/2024/12/22/politics/trump-...   \n",
      "2    https://www.cnn.com/2024/12/22/politics/ivanka...   \n",
      "3    https://www.cnn.com/2024/12/22/politics/joe-ma...   \n",
      "4    https://www.cnn.com/2024/12/22/politics/resist...   \n",
      "..                                                 ...   \n",
      "150  https://www.cnn.com/2024/12/28/us/karen-friedm...   \n",
      "151  https://www.cnn.com/2024/12/28/business/inmate...   \n",
      "152  https://www.cnn.com/2024/12/27/business/costco...   \n",
      "153  https://www.cnn.com/2024/12/27/sport/scottie-s...   \n",
      "154  https://www.cnn.com/2024/12/26/style/nefertiti...   \n",
      "\n",
      "                                               content  \\\n",
      "0    President-elect Donald Trump’s team is assessi...   \n",
      "1    President-elect Donald Trump on Sunday, in his...   \n",
      "2    For Ivanka Trump’s entire adult life, she work...   \n",
      "3    For more on Sen. Joe Manchin of West Virginia,...   \n",
      "4    The question has loomed over Democrats and the...   \n",
      "..                                                 ...   \n",
      "150  The rapper Sean “Diddy” Combs and the suspecte...   \n",
      "151  Graduation day dawns sunny and warm for the fi...   \n",
      "152  Costco is battling an anti-DEI wave with a ste...   \n",
      "153  WhileChristmasDay is the most wonderful time o...   \n",
      "154  One hundred years ago, in a courtyard at the N...   \n",
      "\n",
      "                   category                multi_hot_labels  \\\n",
      "0                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "1                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "2                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "3                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "4                  Politics  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "..                      ...                             ...   \n",
      "150  Society, Entertainment  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0]   \n",
      "151       Business, Society  [0, 1, 0, 0, 0, 0, 0, 0, 1, 0]   \n",
      "152                Business  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
      "153                  Sports  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]   \n",
      "154    Entertainment, World  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0]   \n",
      "\n",
      "                                             tokenized  \n",
      "0    ([tensor(101), tensor(2343), tensor(1011), ten...  \n",
      "1    ([tensor(101), tensor(2343), tensor(1011), ten...  \n",
      "2    ([tensor(101), tensor(2005), tensor(7332), ten...  \n",
      "3    ([tensor(101), tensor(2005), tensor(2062), ten...  \n",
      "4    ([tensor(101), tensor(1996), tensor(3160), ten...  \n",
      "..                                                 ...  \n",
      "150  ([tensor(101), tensor(1996), tensor(10687), te...  \n",
      "151  ([tensor(101), tensor(7665), tensor(2154), ten...  \n",
      "152  ([tensor(101), tensor(3465), tensor(3597), ten...  \n",
      "153  ([tensor(101), tensor(2096), tensor(26654), te...  \n",
      "154  ([tensor(101), tensor(2028), tensor(3634), ten...  \n",
      "\n",
      "[155 rows x 6 columns]\n",
      "tensor([[  101,  2343,  1011,  ..., 10326,  2135,   102],\n",
      "        [  101,  2343,  1011,  ...,  1010,  8398,   102],\n",
      "        [  101,  2005,  7332,  ...,  1998,  2925,   102],\n",
      "        ...,\n",
      "        [  101,  3465,  3597,  ...,  2036,  2056,   102],\n",
      "        [  101,  2096, 26654,  ...,  1521,  1055,   102],\n",
      "        [  101,  2028,  3634,  ...,  7505, 28732,   102]])\n",
      "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# tokenizer cell \n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "max_length = 256\n",
    "\n",
    "def tokenize_text(text):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt' \n",
    "    )\n",
    "    return encoding['input_ids'][0], encoding['attention_mask'][0]\n",
    "\n",
    "# Apply tokenization\n",
    "articles_df['tokenized'] = articles_df['content'].apply(tokenize_text)\n",
    "print(articles_df)\n",
    "\n",
    "input_ids = torch.stack([item[0] for item in articles_df['tokenized']])\n",
    "attention_masks = torch.stack([item[1] for item in articles_df['tokenized']])\n",
    "\n",
    "labels = torch.tensor(np.stack(articles_df['multi_hot_labels'])).float()\n",
    "\n",
    "print(input_ids)\n",
    "print(attention_masks)\n",
    "print(labels)\n",
    "\n",
    "\n",
    "train_inputs, val_inputs, train_labels, val_labels, train_masks, val_masks = train_test_split(\n",
    "    input_ids, labels, attention_masks, test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d0a600-4f73-4745-806b-016a42058ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": train_inputs.numpy(),\n",
    "    \"attention_mask\": train_masks.numpy(),\n",
    "    \"labels\": train_labels.numpy()\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": val_inputs.numpy(),\n",
    "    \"attention_mask\": val_masks.numpy(),\n",
    "    \"labels\": val_labels.numpy()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "068be32c-f7ab-4194-98c1-6ba2abe015d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is PyTorch available: True\n",
      "Is TensorFlow available: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model framework: pt\n"
     ]
    }
   ],
   "source": [
    "# load in the model \n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "import transformers\n",
    "print(\"Is PyTorch available:\", transformers.is_torch_available())\n",
    "print(\"Is TensorFlow available:\", transformers.is_tf_available())\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(categories),\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n",
    "\n",
    "print(\"Model framework:\", model.framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b073f4b-6f47-4f68-bc04-558035c90f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics stuff \n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.3).int().numpy()\n",
    "    \n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa89f9d2-022e-4b6b-a6d1-9f14e88031bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model framework: pt\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"Model framework:\", model.framework)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=51,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dba8839-53e2-4b1d-9250-735eee57f077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ht/fn30gwr12cq2x05xdbq8v2w80000gn/T/ipykernel_23166/4158117230.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 10:44, Epoch 51/51]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.616583</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.555228</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.274441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.508953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.222407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.473552</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.447118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.427085</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.167464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.412349</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.150649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.402306</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.209091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.392939</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.383755</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.290909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.375893</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.345455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.373125</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.256494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.365868</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.360213</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.265152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.357316</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.350120</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.345455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.345918</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.413636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.341913</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.324675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.336837</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.413636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.333372</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.401515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.328936</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.401515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.326891</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.323697</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.438312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.319401</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.317044</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.314533</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.412879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.313581</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.312115</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.308361</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.306882</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.304683</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.427489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.302779</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.300642</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297826</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.297629</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.427489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.296017</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.401515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.295477</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.490260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.293975</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.292688</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.492424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.291114</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.291075</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.290108</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.472944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288565</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288578</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.516234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.288520</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.472944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.287391</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.287025</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.490260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.286617</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.286448</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.446970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.286387</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.490260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.286395</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.535714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=459, training_loss=0.2869148669938896, metrics={'train_runtime': 645.7646, 'train_samples_per_second': 10.978, 'train_steps_per_second': 0.711, 'total_flos': 932664123030528.0, 'train_loss': 0.2869148669938896, 'epoch': 51.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model cell\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1ded5c-b64c-4aab-af99-e71dc8dd913b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.29107508063316345,\n",
       " 'eval_accuracy': 0.4375,\n",
       " 'eval_f1': 0.5357142857142857,\n",
       " 'eval_runtime': 0.3576,\n",
       " 'eval_samples_per_second': 44.747,\n",
       " 'eval_steps_per_second': 2.797,\n",
       " 'epoch': 51.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c0f39f7-0f88-4a19-bec2-8949aa13f661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bert/tokenizer_config.json',\n",
       " './fine_tuned_bert/special_tokens_map.json',\n",
       " './fine_tuned_bert/vocab.txt',\n",
       " './fine_tuned_bert/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b493d86-8cb8-401b-ad24-50b38e31a85f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4d875d-0aa2-4a47-a093-cf815b59dc28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
